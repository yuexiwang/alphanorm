% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/alphanorm.R
\name{alphanorm}
\alias{alphanorm}
\title{fit a sparse model with alpha-norm regularization}
\usage{
alphanorm(x, y, lambda = exp(-10:10), q = 0.5, intercept = TRUE,
  tol = 1e-07, T = 500, nlambda = NULL, trace = FALSE)
}
\arguments{
\item{x}{the design matrix}

\item{y}{the response vector}

\item{lambda}{a vector of lambda values, default as exp(-10:10)}

\item{q}{a numerical value for q, 0<q<=1, with default 0.5}

\item{intercept}{whether the intercept term should be included, TRUE to be included(default), FALSE not to}

\item{tol}{tolerence of convergence condition}

\item{T}{number of maximum iterations for each coefficient}

\item{nlambda}{number of lambda wanted}

\item{trace}{print the process}
}
\value{
An object of S3 class "alphanorm"
\item{\code{x}}{input design matrix}
\item{\code{y}}{input of response vector}
\item{\code{Lambda}}{input of lambda}
\item{\code{q}}{input value of q}
\item{\code{Coefficient}}{matrix coefficients}
\item{\code{Intercept}}{non-penalized intercept(if intercept=TRUE), otherwise, NULL}
\item{\code{df}}{number of nonzero coefficients for each value of lambda}
}
\description{
Fit a alph-norm model with proximal algorithm and coordinate descent
}
\details{
The sequence of models implied by \code{lambda} is solved via coordinate descent.
The objective function is:
\deqn{J(\beta)=1/2 \mbox{RSS}+\lambda*\mbox{penalty}}
Here the penalty is the \eqn{l_q} norm of coefficients, which is \eqn{\sum(|\beta_i|^q), 0<q<=1},
when \eqn{q=1}, it is actually same as lasso
}
\examples{
x<-matrix(rnorm(100*100),100,100)
# Only the first 10 are true predictors
y<-x[,1:10]\%*\%rep(1,10)

# Build a alpha-norm model
alphanorm.obj<-alphanorm(x,y,intercept=FALSE)
# Get coefficients
coef(alphanorm.obj)
# Get fitted values
predict(alphanorm.obj)
# Cross-validation to choose q and lambda
cv.alphanorm(x,y,intercept=FALSE)
# Plot coefficient profile according to log-lambda
plot(alphanorm.obj)
}
\references{
Feng, Guanhao and Polson, Nick and Wang, Yuexi and Xu, Jianeng,
Sparse Regularization in Marketing and Economics (August 20, 2017).
Available at SSRN: \url{https://ssrn.com/abstract=3022856}

Marjanovic, G. and V. Solo (2014). lq sparsity penalized linear regression with cyclic descent.
IEEE Transactions on Signal Processing 62(6), 1464â€“1475.
}
\seealso{
\code{\link{predict.alphanorm}}, \code{\link{coef.alphanorm}}, \code{\link{cv.alphanorm}}, and \code{\link{plot.alphanorm}} methods
}
